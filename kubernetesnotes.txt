kubectl exec -it nginx-pod -- bash
kubectl exec -it my-pod --container con_name1 -- bash

kubectl expose deployment dep_name --type=ClusterIP --name=backend

minikube service service_name --url

kubectl run redis --image=redis123 --dry-run=client -oyaml > redis_pod.yaml
kubectl scale replicaset new-replica-set --replicas=5
kubectl create deployment httpd-frontend --image=httpd:2.4-alpine --replicas=3

#imprative commands
kubectl run nginx-pod --image=nginx:alpine
kubectl run redis --image=redis:alpine --dry-run=client -o yaml > redis_pod.yaml

#open port from pod
kubectl expose pod redis --port=6379 --name redis-service

kubectl create namespace dev-ns

kubectl config set-context --current --namespace=NAMESPACE

#change current config file
export KUBECONFIG=/root/my-kube-config




kubectl create deployment redis-deploy -n dev-ns --image=redis --replicas=2

kubectl run static-busybox --image=busybox --command sleep 1000 --dry-run=client -oyaml > st.yaml

kubectl run httpd --image=httpd:alpine 
kubectl expose pod httpd --type=ClusterIP --port=80 --name httpd

kubectl expose pod messaging --type=ClusterIP --name=messaging-service --port 6379 --target-port 6379 --dry-run=client -o yaml > mes_sv.yaml

kubectl expose deployment hr-web-app --name=hr-web-app-service --type=NodePort --port 8080 --target-port 8080 --dry-run=client -oyaml > hr_svc.yaml
apiVersion: v1
kind: Service
metadata:
  creationTimestamp: null
  labels:
    app: hr-web-app
  name: hr-web-app-service
spec:
  ports:
  - port: 8080
    protocol: TCP
    targetPort: 8080
    nodePort: 30082
  selector:
    app: hr-web-app
  type: NodePort
status:
  loadBalancer: {}



#TAINT and TOLERATION
kubectl taint node node01 spray=mortein:NoSchedule

kubectl taint node controlplane node-role.kubernetes.io/master:NoSchedule-

#affinity
kubectl label node node01 app=blue

#Configmap creating
kubectl create configmap webapp-config-map --from-literal=APP_COLOR=darkblue

#SECRET
kubectl create secret generic db-secret --from-literal=DB_Host=sql01 --from-literal=DB_User=root --from-literal=DB_Password=password123


apt update
apt-cache madison kubeadm


apt-get update && \
apt-get install -y --allow-change-held-packages kubeadm=1.20.0-00

kubeadm upgrade plan

kubeadm upgrade apply v1.20.0


#NODE UZERINDEKI PODLARI(REPLICASET ILE OLUSTURULMUSSA) BOSALTMA
#MANUEL OLUSTURULAN BIR POD ISE NODE TASIMA ISLEMINDE YOK OLUR. 
kubectl drain node01 --ignore-daemonsets


#NODE u devreye alma
kubectl uncordon node01

#NODE u dugumleme
kubectl cordon 

apt update
apt install kubeadm=1.20.0-00 
kubeadm upgrade node


kubeadm init --apiserver-advertise-address $(hostname -i) --pod-network-cidr 10.5.0.0/16

kubectl apply -f https://raw.githubusercontent.com/cloudnativelabs/kube-router/master/daemonset/kubeadm-kuberouter.yaml

kubeadm token create --print-join-command

kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
kubectl apply -f https://raw.githubusercontent.com/flannel-io/flannel/master/Documentation/kube-flannel.yml

apt install kubelet=1.20.0-00 -y

systemctl restart kubelet

kubectl exec webapp -- cat /log/app.log

kubectl get ingress --all-namespaces

kubectl create ingress ingress-test --rule="wear.my-online-store.com/wear*=wear-service:80"

kubectl create configmap nnginx-configuration  -n ingress-space
kubectl create serviceaccount ingress-serviceaccount -n ingress-space 

kubectl config set-context --current --namespace=alpha

kubectl get nodes -o=jsonpath='{.items[*].metadata.name}' > /opt/outputs/node_names.txt

kubectl get nodes -o jsonpath='{.items[*].status.nodeInfo.osImage}'

kubectl config view --kubeconfig=my-kube-config -o jsonpath="{.users[*].name}" > /opt/outputs/users.txt

kubectl get pv --sort-by=.spec.capacity.storage > /opt/outputs/storage-capacity-sorted.txt


persistent volume 

apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-analytics
spec:
  capacity:
    storage: 100Mi
  accessModes:
    - ReadWriteOnce
  hostPath:
	path: /pv/data-analytics
	
	
	
	etcdctl backup
    etcdctl cluster-health
    etcdctl mk
    etcdctl mkdir
    etcdctl set
	
	export ETCDCTL_API=3
	etcdctl snapshot save /opt/etcd-backup.db --endpoints=https://10.21.101.6:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key
	
	export ETCDCTL_API=3
	etcdctl snapshot save 
    etcdctl endpoint health
    etcdctl get
    etcdctl put
	
	
	--cacert /etc/kubernetes/pki/etcd/ca.crt     
    --cert /etc/kubernetes/pki/etcd/server.crt     
    --key /etc/kubernetes/pki/etcd/server.key
	
	    kubectl exec etcd-master -n kube-system -- sh -c "ETCDCTL_API=3 etcdctl get / --prefix --keys-only --limit=10 --cacert /etc/kubernetes/pki/etcd/ca.crt --cert /etc/kubernetes/pki/etcd/server.crt  --key /etc/kubernetes/pki/etcd/server.key" 

ETCDCTL_API=3 etcdctl  --data-dir /var/lib/etcd-from-backup \
snapshot restore /opt/snapshot-pre-boot.db


root@controlplane:~# ETCDCTL_API=3 etcdctl --endpoints=https://[127.0.0.1]:2379 \
--cacert=/etc/kubernetes/pki/etcd/ca.crt \
--cert=/etc/kubernetes/pki/etcd/server.crt \
--key=/etc/kubernetes/pki/etcd/server.key \
snapshot save /opt/snapshot-pre-boot.db


ETCDCTL_API=3 etcdctl  --data-dir /var/lib/etcd-from-backup \
snapshot restore /opt/snapshot-pre-boot.db

root@controlplane:~# kubectl get pv --sort-by=.spec.capacity.storage
NAME       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   REASON   AGE
pv-log-4   40Mi       RWX            Retain           Available                                   48m
pv-log-1   100Mi      RWX            Retain           Available                                   48m
pv-log-2   200Mi      RWX            Retain           Available                                   48m
pv-log-3   300Mi      RWX            Retain           Available                                   48m

kubectl get pv --sort-by=.spec.capacity.storage -o=custom-columns=NAME:.metadata.name,CAPACITY:.spec.capacity.storage > /opt/outputs/pv-and-capacity-sorted.txt
NAME       CAPACITY
pv-log-4   40Mi
pv-log-1   100Mi
pv-log-2   200Mi
pv-log-3   300Mi

kubectl exec webapp -- cat /log/app.log

kubectl config view --kubeconfig=my-kube-config -o jsonpath="{.contexts[?(@.context.user=='aws-user')].name}" > /opt/outputs/aws-context-name

kubectl get deployments --sort-by=.metadata.name -o=custom-columns=DEPLOYMENT:.metadata.name,CONTAINER_IMAGE:.spec.template.spec.containers.*.image,READY_REPLICAS:.status.readyReplicas,NAMESPACE:.metadata.namespace

ip addr  add 10.0.0.1/30 dev eth1

ip addr  add 10.0.0.2/30 dev eth1
ip addr  add 10.2.0.1/30 dev eth6

ip addr  add 10.2.0.2/30 dev eth5

iptables -L -t nat | grep db-service

kubectl run super-user-pod --image=busybox:1.28 --dry-run=client -oyaml > superuser.yaml --command sleep 3600

kubectl apply -f roll.yaml --record



kubectl set image deployment/nginx-deploy  nginx=nginx:1.16 --record    deployment/deployment adı container adı=imaj adı:version

kubectl edit deployments nginx-deploy --record

kubectl rollout history deployment nginx-deploy

openssl genrsa -out akshay.key 2048

openssl req -newkey akshay.key -subj "/CN=akshay" -out akshay.csr

kubectl get csr

kubectl certificate approve akshay

openssl x509 -in akshay.csr -text -noout

kubectl config view

kubectl config view --kube-config=my-custom-config

kubectl config use-context prod-user@production
 
kubectl config --kubeconfig=/root/my-kube-config use-context research

kubectl config -h

export KUBECONFIG=/root/my-kube-config YENI CONFIG DOSYASI GOSTERME

kubectl describe pod kube-apiserver-controlplane -n kube-system | grep authorization-mode

kubectl describe roles kube-proxy -n kube-system

kubectl describe rolebinding kube-proxy -n kube-system

kubectl get pod --as dev-user

kubectl create role developer --namespace=default --verb=list,create,delete --resource=pods

kubectl create rolebinding dev-user-binding --namespace=default --role=developer --user=dev-user

kubectl edit role developer -n blue

kubectl create role deploy-role --namespace=blue --verb=create --resource=deployment

kubectl get clusterroles --no-headers | wc -l

kubectl get clusterroles --no-headers -o json | jq '.items | length'

kubectl create clusterrole node-ad --verb=get,watch,list,create,delete --resource=nodes

kubectl create clusterrolebinding node-rlb --clusterrole=node-ad --user=michelle

kubectl create clusterrole storage-admin --verb=* --resource=persistentvolumes,storageclasses

kubectl create clusterrolebinding michelle-storage-admin --clusterrole=storage-admin  --user=michelle

kubectl create serviceaccount dashboard-sa

kubectl create secret docker-registry private-reg-cred --docker-username=dock_user --docker-password=dock_password --docker-server=myprivateregistry.com:5000 --docker-email=dock_user@myprivateregistry.com

imagePullSecrets:
      - name: private-reg-cred
	  
kubectl exec ubuntu-sleeper -- whoami

vim ~/.vimrc
autocmd FileType yaml setlocal et ts=2 ai sw=2 nu sts=0
syntax on

grep -i staticpod /var/lib/kubelet/config.yaml

kubectl run  static-busybox --image=busybox --dry-run=client -oyaml --command -- sleep 1000 > deneme3.yaml

#static config file bulma
kubectl get pods --all-namespaces -o wide  | grep static-greenbox
ssh node01
ps -ef | grep kubelet | grep config.yaml
grep -i staticpod /var/lib/kubelet/config.yaml
output > staticPodPath: /etc/just-to-mess-with-you

rm /etc/just-to-mess-with-you/greenbox.yaml

#create another scheduler
- --leader-elect=false
- --port=10282
- --scheduler-name=my-scheduler
- --secure-port=0

spec:
  schedulerName: deneme

kubectl logs etcd-controlplane -n kube-system | grep -i version


